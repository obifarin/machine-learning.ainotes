{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Machine Learning: An Overview\n",
    "\n",
    "Author: Olatomiwa Bifarin. <br>\n",
    "PhD Candidate Biochemistry and Molecular Biology <br>\n",
    "@ The University of Georgia\n",
    "\n",
    "_This is a draft copy, a work in progress_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Content\n",
    "\n",
    "1.  [Definition](#1) <br>\n",
    "2.  [ML Problems](#2) <br>\n",
    "3.  [ML Concepts](#3) <br>\n",
    "4.  [Applications](#4) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definition\n",
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have read many definitions for machine learning, however none is as expantiative (and pitty) as Tom Michell's namely: <br>\n",
    "\n",
    "`\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\"`\n",
    "\n",
    "An example: <br> \n",
    "Task `T`: Classify a subject as having Parkinson's disease or not <br>\n",
    "Experience `E`: Voice measurements data and corresponding labels. <br>\n",
    "Performance `P`: Accuracy of classification. \n",
    "\n",
    "Mathematically speaking, the goal is to figure out an approximating function $f$\n",
    "\n",
    "$$f:X\\rightarrow Y$$\n",
    "Where $X$ are the features and labels and $Y$ the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ML Problems\n",
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as you might have guessed, there are different kind of task `T`, and the different kind of task `T` would be solved differently. <br>\n",
    "Take the Parkinson's disease above. This is what is called a <mark>classification</mark> problem, and it is expressed here in it's simplest form: binary classification. Let's say a few things about this (and similar) problems before I proceed to some other machine learning problems.  \n",
    "$X$ and $Y$ are defined as follows: \n",
    "\n",
    "$$X: [(x_{1},y_{1}),(x_{2},y_{2})...(x_{n},y_{n})]$$\n",
    "$$ Y = (y_{1},...,y_{n}) \\in {0,1} $$\n",
    "\n",
    "Once a prediction is made, we want to capture if we are doing well. We do this via something called the `loss function` or sometimes `cost function`. For a binary classification problem, it's defined as such: \n",
    "\n",
    "$$ c(y, f(x)) = \n",
    "\\begin{cases}\n",
    "0 & \\text{ if } y=f(x) \\\\\n",
    "1 & \\text{ if } y \\neq f(x)\n",
    "\\end{cases}$$\n",
    "\n",
    "This kind of classification is called `supervised classification`: supervised because we have the labels. It doesn't take much to figure out that the other kind could be, and indeed is called `unsupervised classification`. And in this case we are no labels. This is a <mark>clustering</mark> problem. As the name suggests we use algorithms that clusters samples based on a similarity heuristics. <br> \n",
    "\n",
    "In the above examples, what we are attempting to predict is a qualitative variable. Now, check this out: Given the quantified metabolites in a cancer patient's urine, can we predict the size of tumor. The target variable here is a quantitative variable. This is called a <mark>regression</mark> problem - as opposed to a qualitative variable, we sort to predict a quantitative variable. However classification and regression algorithms are very much connected, for example many classification algorithms predicts classes by first estimating probabilities of classes, in this way they behave like a regression method. Also, in logistic regression for example a regression algorithm actually predicts a quantitative variable. \n",
    "\n",
    "Another popular kind of a machine learning problem is called <mark>ranking</mark>. And here is an example: I type the following words into a search engine like google _what are ranking machine learning problems?_ To solve the problem effectively, the algorithm will have to give me a ranked output of what I might like. This turns out to be an interesting machine learning problem. Other kinds of ML problems incude <mark>reinforcement learning</mark>, <mark>representation learning</mark>, <mark>collaborative filtering</mark>, <mark>anomaly detection</mark> e.t.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ML Concepts\n",
    "<a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of what use is a car that cannot move. Of what use is a machine algorithms that cannot learn (i.e. generalize). And a machine learning generalize well, when it does not `underfit` or `overfit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inductive Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In philosophy, inductive reasoning is when you come to a conclusion, using a premise that does `NOT` ascertain the conclusion (as opposed to what you get in deductive reasoning.) <br> \n",
    "\n",
    "An example: \n",
    "\n",
    "<center>All swans I have seen are black</center>\n",
    "<center>Therefore, all swans are black</center>\n",
    "\n",
    "Now, recall that this is all we do in machine learning: train data with a machine learning algorithm (_premise_), and argue that such machine learning algorithm generalize to the test data (_conclusion_). Aha! And the different kind of ways machine learning algoritm does this is what is called the <mark>induction bias</mark>, the assumptions they hold.<br> \n",
    "\n",
    "Here are examples of a few: \n",
    "\n",
    "| ML Algorithm| Inductive Bias |\n",
    "| --- | --- |\n",
    "| ___Decision Trees___ | Shorter trees are desired, weight are given to fewer features |\n",
    "| ___k-Nearest Neighbors___ | 1)Closer samples (in the euclidean space, defined by k-NN) are more likely to be same, 2) All samples are equally important |\n",
    "| ___Support Vector Machines___ | Classes are separable by a hyperplane with some margin |\n",
    "| ___Linear Regression___ | A linear relationship between the features and the response variable |\n",
    "| ___Naive Bayes___ | Inputs are independent of each other |\n",
    "| ___Perceptron___ |  |\n",
    "| ___Neural Networks___ |  Number of hidden units?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Inductive_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization and Regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML Applications\n",
    "<a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "\n",
    "- Tom Michell's Machine Learning Pg?\n",
    "- Wikipedia, Induction bias\n",
    "- http://www.lauradhamilton.com/inductive-biases-various-machine-learning-algorithms\n",
    "- Introduction to Statistical Learning (Chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
