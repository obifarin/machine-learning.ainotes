{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>\n",
    "<img src=\"datasets/AInote_logo.jpeg\" height=\"1000\" width=\"700\" />\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "Author: Olatomiwa Bifarin. <br>\n",
    "PhD Candidate Biochemistry and Molecular Biology <br>\n",
    "@ The University of Georgia\n",
    "\n",
    "_This is a draft copy, a work in progress_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Specialization Coursera Course 1 Code especially Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Content\n",
    "\n",
    "1.  [Perceptrons](#1) <br>\n",
    "2.  [Introduction to Neural Network](#2) <br>\n",
    "    2.1 [NN Architecture](#2.1) <br>\n",
    "    2.2 [MLP and XOR](#2.2) <br>\n",
    "    2.3 [Activating Functions](#2.3) <br>\n",
    "3.  [Training Neural Network](#3) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron\n",
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Neural Networks\n",
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nucleic acid is to the genome as perceptrons is to the neural network. In order words, perceptrons are the basic unit of a neural network. A single layered 'network' is a perceptron (inputs goes directly into output), while a multi-layered network gives us the neural network. There is another important distinction to be made: while perceptrons finds a linear decision boundary for classifications (perceptron is a linear model), neural networks are capable of drawing non-linear decision boundaries, as such more complex, and potentially more useful. Inshort, neural nets are universal function approximator, as they as sooo flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NN Architecture\n",
    "<a id=\"2.1\"></a>\n",
    "\n",
    "<img src=\"datasets/nn2.jpg\" height=\"200\" width=\"350\" />\n",
    "\n",
    "(Image Credit, change image, or draw your own)\n",
    "\n",
    "Displayed above is a representation of a multilayer perceptron (MLP). We have the input nodes, the hidden layer, and the output nodes. For each edges, a weight $w$ is ascribed, each layer has it's own bias $b$, and finally an `activation function` $\\sigma$ at each node. As stated above, the difference between the computation of a perceptron and that at the node of a neural network is the presence of the activation function $\\sigma$ that gives its the non-linear boundary property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Some Mathematical definition:_\n",
    "\n",
    "$$ N^{1} = \\sigma ( w^{1}.N^{0} + b^{1})$$\n",
    "\n",
    "$$ N^{2} = \\sigma ( w^{2}.N^{1} + b^{2})$$\n",
    "\n",
    "$$ N^{2} = \\sigma ( w^{2}.\\sigma ( w^{1}.N^{0} + b^{1}) + b^{2})$$\n",
    "\n",
    "Where $N^{0}$ is the input vector, $N^{2}$ is the output vector, $b$ is the vector of biases, and $w$ represent the weight matrices. The `number of weight in a layer` is the product of the input and the output neuron in that layer, in the example above, the first layer has  $4 X 3$ (12) weights, while the second layer as a $3 X 2$ (6) weights. While the number of biases are `the number of hidden layer units` and output units. in the example above, it is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP and XOR\n",
    "<a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Activation Functions\n",
    "<a id=\"2.3\"></a>\n",
    "\n",
    "https://github.com/obifarin/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Neural Networks\n",
    "<a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural networks involves using gradient descent and the multivariate calculus chain rule to update the values of weights and biases in the model in order to carry out classification of inputs (training data). The training method is called back propagation. _Back_ propagation because it observes the output and then propagates backward into the neural network to update weight and biases.  \n",
    "\n",
    "Let's take the example above, with 4 input neurons and 3 hidden neurons, and 2 output neurons. This gives 18 weights and 5 bias elements all together. The goal now is to get the _right_ $w$ and $b$ that will give the best classification results over the training data. Here comes in loss function, lets define a loss function: \n",
    "\n",
    "$$ L = \\sum_{i} \\frac{1}{2}(y_{i} - N^{out})^2$$\n",
    "\n",
    "A square loss function $L$ where $y_{i}$ is the ground truth and $N^{out}$ is the current output from the network. What we want to do now is to fine-tune the hyperparameters. And you ask how? We said it earlier, via gradient descent and the multivariate calculus chain rule. We want to find the minimum! So we take a partial derivation of $L$ with respect to both $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.19512&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "-  CIML, Chapter 10\n",
    "-  Hands on Machine learning (Read Chapter on ANN)\n",
    "-  Coursera Course (Lectures on Artificial Neural Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
