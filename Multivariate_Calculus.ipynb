{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Calculus\n",
    "\n",
    "Author: Olatomiwa Bifarin <br>\n",
    "\n",
    "_This is  a Draft Copy_\n",
    "\n",
    "## Notebook Outline\n",
    "\n",
    "1. [Introduction](#1)\n",
    "2. [Derivatives](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Resources_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Multivariable calculus from coursera\n",
    "-  __[Mathematical Python](https://www.math.ubc.ca/~pwalls/math-python/)__\n",
    "-  https://mml-book.github.io/book/mml-book_printed.pdf\n",
    "-  https://gwthomas.github.io/docs/math4ml.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the goal is to find (and learn) certain pattern in the input data in order to make certain predictions. __Cost functions__ are an integral part of this process as it measures how poorly the model is predicting. The goal of calculus is to minimize this cost, by which prediction is improved. Examples can be found in regression problems (curve fitting), and neural networks. The mathematical tools for this tasks will be the focus in this notebook. \n",
    "\n",
    "In summary, the gradient measures how the output changes as the input parameters changes, as such we want the function's gradients to tend towards the direction of the steepest ascents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Derivatives\n",
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $ x $: \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x+\\Delta x) - f(x)}{\\Delta x}\n",
    "$$\n",
    "\n",
    "The derivative gives the gradient, which tends towards the direction of the steepest ascents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Derivatives Function___ <br>\n",
    "Show in graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Differentiation Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Sum Rule___ \n",
    "\n",
    "$$\n",
    "\\frac{d ({f(x)+g(x)})}{d {x}} =\n",
    "\\frac{d {f(x)}}{d x} +\n",
    "\\frac{d {g(x)}}{d x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Power Rule___ \n",
    "\n",
    "$$\n",
    "f(x) = ax^b\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(x) = abx^{b-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Product Rule___ \n",
    "\n",
    "$$\n",
    "A(x) = f(x)g(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "A'(x) = f(x)g'(x) + g(x)f'(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Chain Rule___ \n",
    "\n",
    "If $ h=h(p)$ and $ p=p(m) $:\n",
    "\n",
    "$$\n",
    "\\frac{d  h}{d \\mathbf m } = \\frac{d \\mathbf h}{d \\mathbf p } X \\frac{d \\mathbf p}{d \\mathbf m }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Quotient Rule___\n",
    "\n",
    "$$\n",
    "{[\\frac{f(x)}{g(x)}]}' = \\frac{g(x)f'(x) - f(x)g'(x)}{[g(x)]^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Examples and Special cases___\n",
    "\n",
    "$$ f(x) = \\frac{1}{x} $$\n",
    "$$ f'(x) = \\frac{-1}{x^2} $$\n",
    "\n",
    "\n",
    "$$ f(x) = e^x $$\n",
    "$$ f'(x) = e^x $$\n",
    "$$ f''(x) = e^x $$\n",
    "$$ ... $$\n",
    "\n",
    "$$ f(x) = sin(x) $$\n",
    "$$ f'(x) = cos(x) $$\n",
    "$$ f''(x) = -sin(x) $$\n",
    "$$ f'''(x) = -cos(x) $$\n",
    "$$ f''''(x) = sin(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Derivatives: Pythonic Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Derivatives: Partial and Total Derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jacobians and Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. vectors of derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. The sand pit: Intuition for jacobian and gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. The Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multivariate Chain Rule and its Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Training neural network (change values and annotations)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set the state of the network\n",
    "σ = np.tanh\n",
    "w1 = 1.3\n",
    "b1 = -0.1\n",
    "\n",
    "# Then we define the neuron activation.\n",
    "def a1(a0) :\n",
    "  z = w1 * a0 + b1\n",
    "  return σ(z)\n",
    "\n",
    "# Experiment with different values of x below.\n",
    "x = 0\n",
    "a1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function.\n",
    "sigma = np.tanh\n",
    "\n",
    "# Let's use a random initial weight and bias.\n",
    "W = np.array([[-0.94529712, -0.2667356 , -0.91219181],\n",
    "              [ 2.05529992,  1.21797092,  0.22914497]])\n",
    "b = np.array([ 0.61273249,  1.6422662 ])\n",
    "\n",
    "# define our feed forward function\n",
    "def a1 (a0) :\n",
    "  # Notice the next line is almost the same as previously,\n",
    "  # except we are using matrix multiplication rather than scalar multiplication\n",
    "  # hence the '@' operator, and not the '*' operator.\n",
    "  z = W @ a0 + b\n",
    "  # Everything else is the same though,\n",
    "  return sigma(z)\n",
    "\n",
    "# Next, if a training example is,\n",
    "x = np.array([0.1, 0.5, 0.6])\n",
    "y = np.array([0.25, 0.75])\n",
    "\n",
    "# Then the cost function is,\n",
    "d = a1(x) - y # Vector difference between observed and expected activation\n",
    "C = d @ d # Absolute value squared of the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In this assignment, you will train a neural network to draw a curve by implementing backpropagation by the chain rule to calculate Jacobians of the cost function.\n",
    "\n",
    "The neural network will then be trained using a (pre-implemented) stochastic steepest descent method, and will draw a series of curves to show the progress of the training.\n",
    "\n",
    "You will have to copy and edit pre-written python code in this assessment, but will not need to write new code. It should be tractable even to learners with little coding experience, so long as you don't panic about the code and work through each line diligently.\n",
    "\n",
    "Best of luck!!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Taylor series and linearisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=3d6DsjIBzJ4&t=165s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Maclaurin Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Taylor Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Multivariable Taylor Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Newton-Raphson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Multivariable calculus from coursera\n",
    "-  __[Mathematical Python](https://www.math.ubc.ca/~pwalls/math-python/)__\n",
    "-  https://mml-book.github.io/book/mml-book_printed.pdf\n",
    "-  https://gwthomas.github.io/docs/math4ml.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
