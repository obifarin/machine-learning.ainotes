{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>\n",
    "<img src=\"datasets/AInote_logo.jpeg\" height=\"1000\" width=\"500\" />\n",
    "\n",
    "# [Decision] Trees and [Random] Forest\n",
    "\n",
    "Author: Olatomiwa Bifarin. <br>\n",
    "PhD Candidate Biochemistry and Molecular Biology <br>\n",
    "@ The University of Georgia\n",
    "\n",
    "_This is a draft copy, a work in progress_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another very intuitive machine learning algorithm. <br> In one sentence: A decision tree is a tree describing how a decision is made. No more, no less. \n",
    "<img align='center' src='datasets/decision_tree_quora.png' width=50%/>\n",
    "Source: __[link](https://www.quora.com/What-is-an-intuitive-explanation-of-a-decision-tree)__ (_Make my own example in ppt or draw in latex(google)_)\n",
    "\n",
    "Take the example I gave above, if my parent is visiting, go to the cinema. If they are not, I ask another question: is it rainy or cloudy? If it is the case that it is rainy, I stay indoors. If it is not, I go shopping. And as you can see using this simplified example: when a machine learning algorithm predicts via a decision tree, one benefit is that it's easy to interpret how the algorithm come to a decision. In order words, interpretability is easy. This is not the case for many, many machine learning algorithm, as (when) they come to a great decision (prediction), but it's hard - if possible at all - to interpret how the algorithm come to a decision. This is what is referred to by many as a learning algorithms being a `black box`. \n",
    "\n",
    "_Architecture, and nomenclature:_ A decision tree is an inverted tree, starting with the `root node`, this is followed by the `internal nodes` (i.e. nodes that split into other nodes). Finally, at the bottom of the tree is the leaf, or sometimes called the `leaf node`. In the example above, _parent visiting?_ is the root node, _weather_ is the internal node, and _go to cinema, stay in, go shopping_ are the leaf nodes. As you can see here, the internal node test a feature, while the leaf node is a decision (or classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Decision Tree Expressiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at how expressive decision trees can be using some Boolean and set functions. \n",
    "-  AND \n",
    "\n",
    "X `AND` Y\n",
    "\n",
    "_Draw figure with latex_\n",
    "\n",
    "-  OR \n",
    "\n",
    "X `OR` Y\n",
    "\n",
    "_Draw figure with latex_\n",
    "\n",
    "-  XOR \n",
    "\n",
    "X `XOR` Y\n",
    "\n",
    "_Draw figure with latex_\n",
    "\n",
    "-  Sets\n",
    "\n",
    "$ A \\cap B \\cup (C \\cap \\neg D) $\n",
    "\n",
    "_Draw figure with latex_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "-  A course in machine learning (CIML) Decision Tree Chapter\n",
    "-  ISL Decision Tree and Random Forest Chapter\n",
    "-  STAT 6250: Statistical learning Week 12 Part 2 lecture\n",
    "-  Stat Quest with Josh Starmer videos on decision trees and forest\n",
    "-  mlcourse.ai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
