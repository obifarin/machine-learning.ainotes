{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center>\n",
    "<img src=\"datasets/AInote_logo.jpeg\" height=\"1000\" width=\"700\" />\n",
    "\n",
    "# [Decision] Trees and [Random] Forest\n",
    "\n",
    "Author: Olatomiwa Bifarin. <br>\n",
    "PhD Candidate Biochemistry and Molecular Biology <br>\n",
    "@ The University of Georgia\n",
    "\n",
    "_This is a draft copy, a work in progress_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another very intuitive machine learning algorithm. <br> In one sentence: A decision tree is a tree describing how a decision is made. No more, no less. \n",
    "<img align='center' src='datasets/decision_tree_quora.png' width=50%/>\n",
    "Source: __[link](https://www.quora.com/What-is-an-intuitive-explanation-of-a-decision-tree)__ (_Make my own example in ppt or draw in latex(google)_)\n",
    "\n",
    "Take the example I gave above, if my parent is visiting, go to the cinema. If they are not, I ask another question: is it rainy or cloudy? If it is the case that it is rainy, I stay indoors. If it is not, I go shopping. And as you can see using this simplified example: when a machine learning algorithm predicts via a decision tree, one benefit is that it's easy to interpret how the algorithm come to a decision. In order words, interpretability is easy. This is not the case for many, many machine learning algorithm, as (when) they come to a great decision (prediction), but it's hard - if possible at all - to interpret how the algorithm come to a decision. This is what is referred to by many as a learning algorithms being a `black box`. \n",
    "\n",
    "_Architecture, and nomenclature:_ A decision tree is an inverted tree, starting with the `root node`, this is followed by the `internal nodes` (i.e. nodes that split into other nodes). Finally, at the bottom of the tree is the leaf, or sometimes called the `leaf node`. In the example above, _parent visiting?_ is the root node, _weather_ is the internal node, and _go to cinema, stay in, go shopping_ are the leaf nodes. As you can see here, the internal node test a feature, while the leaf node is a decision (or classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Decision Tree Expressiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at how expressive decision trees can be using some Boolean and set functions. \n",
    "-  AND \n",
    "\n",
    "X `AND` Y\n",
    "\n",
    "_Draw figure with latex_\n",
    "\n",
    "-  OR \n",
    "\n",
    "X `OR` Y\n",
    "\n",
    "_Draw figure with latex_\n",
    "\n",
    "-  XOR \n",
    "\n",
    "X `XOR` Y\n",
    "\n",
    "_Draw figure with latex_\n",
    "\n",
    "-  Sets\n",
    "\n",
    "$ A \\cap B \\cup (C \\cap \\neg D) $\n",
    "\n",
    "_Draw figure with latex_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a machine learning problem, the goal is to figure out an approximating function $f$\n",
    "\n",
    "$$f:X\\rightarrow Y$$\n",
    "Where $X$ are the features and labels and $Y$ the predictions, and are defined as follows (in the case of a binary classification): \n",
    "\n",
    "$$X: [(x_{1},y_{1}),(x_{2},y_{2})...(x_{n},y_{n})]$$\n",
    "$$ Y = (y_{1},...,y_{n}) \\in {0,1} $$\n",
    "\n",
    "To solve the problem, the first question that comes to mind is, how do we start? i.e. how do we choose features? To answer this question, we perhaps what to ask yet a slightly different question: how do we know if we have choosen a good feature? A good feature by definition is a feature that lead to the right prediction. In information theory, there are many approaches we could use, and one popular one is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our intuition on this matter at hand isn't going to hard at all. In thermodynamics, entropy is a measure of chaos in a system, and when we bring this idea into information theory: chaos becomes uncertainity; believe me, it does. Before I show this, let us define entropy mathematically: \n",
    "\n",
    "$$X = {x_{1},x_{2},...x_{n}}$$\n",
    "\n",
    "$$ \\Large H(x) = -\\sum_{i=1}^{n} p_{i}\\log_{2}(p_{i}) $$\n",
    "\n",
    "$$ \\text{Where } p_{i} = P(X = x_{i}) $$\n",
    "\n",
    "Now to show you what I mean, take an urn. This particular urn contains 3 red balls and 2 blue balls, and what is the entropy in this system? \n",
    "\n",
    "$$ - (0.6 \\log_{2}0.6 + 0.4 \\log_2{0.4}) = 0.97$$\n",
    "\n",
    "$$ \\text{Where } p(\\text{red balls}) = 0.6 \\text{ and } p(\\text{blue balls}) = 0.4 $$\n",
    "\n",
    "Remove one blue ball from that damn urn, and what is the entropy again? \n",
    "\n",
    "$$ -(0.75 \\log_{2}0.75 + 0.25 \\log_2{0.25}) = 0.82$$\n",
    "\n",
    "$$ \\text{Where } p(\\text{red balls}) = 0.75 \\text{ and } p(\\text{blue balls}) = 0.25 $$\n",
    "\n",
    "Now remove that last blue ball, and let see what we have got:\n",
    "\n",
    "$$ -1 \\log_{2}1 = 0$$\n",
    "\n",
    "$$ \\text{Where } p(\\text{red balls}) = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 0.97 to 0.82, and then to 0; alas entropy = uncertainity. Now if you think very carefully about this, as the certainity of picking a red ball increases, the entropy decreases. Now let's make the link. What we WANT in a decision tree is to reduce the uncertainity (entropy) as we go down the tree. Isn't it? Thats the leaf! Thats supposed to be the prediction! The reduction in entropy as we go down a good decision tree is called `information gain`. If we go back to our initial question:  _how do we know if we have choosen a good feature as a root node or to split a node?_ Friends, we have just figured out the answer. `It is the feature with the highest information gain.`\n",
    "\n",
    "We define it as the following: \n",
    "\n",
    "$$IG(x) = Info(D) - Info_{x}(D) $$\n",
    "\n",
    "$$ \\text{Where: } Info(D) = -\\sum_{i=1}^{n} p_{i}\\log_{2}(p_{i})$$\n",
    "\n",
    "$$Info_{x}(D) = -\\sum_{i=1}^{j} \\frac{|D_{i}|}{|D|} \\text{ X } Info(D_{i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Decision Tree: A Simplified Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "-  A course in machine learning (CIML) Decision Tree Chapter\n",
    "-  ISL Decision Tree and Random Forest Chapter\n",
    "-  STAT 6250: Statistical learning Week 12 Part 2 lecture\n",
    "-  Stat Quest with Josh Starmer videos on decision trees and forest\n",
    "-  mlcourse.ai \n",
    "-  https://medium.com/analytics-vidhya/intuition-information-entropy-cae61cb159a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
